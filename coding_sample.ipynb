{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "Hi! My name is Fei Liang. Thank you for being interested in my coding skills. In this coding sample, I present a project that solves a natural language processing problem using Python.\n",
    "\n",
    "The project I have developed demonstrates my proficiency in Python and NLP techniques. The goal of the project is to analyze a collection of text data and extract meaningful information from it. Specifically, the project involves preprocessing the text data, performing feature extraction, and building models using machine learning algorithms.\n",
    "\n",
    "I hope that this coding sample provides a glimpse into my skills and abilities as a data scientist.\n",
    "\n",
    "Please note that some materials and code have been removed from this coding sample due to non-disclosure agreements (NDA) with previous clients. If you have any questions about the missing portions or would like more information about my experience, please don't hesitate to contact me. I would be happy to discuss my work in more detail with you."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "In this part, we start by cleaning the text data by removing special characters, punctuation, and other non-alphanumeric characters. We also remove any stop words, which are commonly used words that don't provide much value in our analysis, such as \"the\", \"and\", and \"a\". After cleaning the text data, we perform lemmatization, which involves reducing words to their root form, to further reduce the number of unique words in the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/liangfei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/liangfei/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/liangfei/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liangfei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/liangfei/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all the packages needed in the data preprocessing part of this project\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "# This dataset contains the responses collected from the survey data. It has the following columns:\n",
    "# 'TA', 'Brand', 'Response', 'Month', 'Year', 'Quarter'\n",
    "# This project mainly focus on the 'Response' column which contains free text feedbacks\n",
    "dataset = pd.read_csv('Dataset.csv')\n",
    "dataset.columns = ['TA', 'Brand', 'Response', 'Month', 'Year', 'Quarter']\n",
    "dataset = dataset.drop(columns=['Quarter'])\n",
    "# dataset.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Replace non-alphanumeric characters, punctuation, technical terms, and abbreviations to the original form using a dictionary created before."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Read the word dictionary from a json file\n",
    "with open('word_dict.json', 'r') as file:\n",
    "    word_dict = json.load(file)\n",
    "\n",
    "def sub_word(text, dictionary):\n",
    "    '''\n",
    "    Substitute the words that match the keys of the dictionary\n",
    "    :param text: The text string to be modified\n",
    "    :param dictionary: The word dictionary\n",
    "    :return: A modified text string\n",
    "    '''\n",
    "    # Convert everything to lower case before we perform the transformation\n",
    "    text = str(text).lower()\n",
    "\n",
    "    for key, value in dictionary.items():\n",
    "        text = re.sub(key, value, text)\n",
    "\n",
    "    # strip the leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Apply the sub_word function to the Response column of the dataset\n",
    "dataset['Response_Basic'] = dataset['Response'].apply(sub_word, args=(word_dict,))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, remove all the stopwords from the `Response_Basic` column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# get the stopwords from the `nltk` package\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    This function remove all the stopwords in a text string\n",
    "    :param text: a text string to be modified\n",
    "    :return: a cleaned text string\n",
    "    '''\n",
    "    # tokenize the text into a list of words\n",
    "    word_list = word_tokenize(text)\n",
    "    # filter out the words in the stop_words list\n",
    "    cleaned_word_list = filter(lambda word: word not in stop_words, word_list)\n",
    "    # join the remaining words back to a string\n",
    "    return ' '.join(cleaned_word_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Apply the remove_stopwords function to the Response_Basic column of the dataset\n",
    "dataset['Response_Cleaned'] = dataset['Response_Basic'].apply(remove_stopwords)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, lemmatize the word to reduce the number of different words we have"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    '''\n",
    "    This function lemmatize all the words in a text string\n",
    "    :param text: a text string to be modified\n",
    "    :return: a text string with all words lemmatized\n",
    "    '''\n",
    "    # tokenize the text into a list of words\n",
    "    word_list = word_tokenize(text)\n",
    "    # filter out the words in the stop_words list\n",
    "    lemmatized_word_list = list(map(lemmatizer.lemmatize, word_list))\n",
    "    # join the remaining words back to a string\n",
    "    return ' '.join(lemmatized_word_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Apply the remove_stopwords function to the Response_Basic column of the dataset\n",
    "dataset['Response_Processed'] = dataset['Response_Cleaned'].apply(lemmatize)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, remove all the rows with missing values in the `Response_Processed` column. Output the dataset to a csv file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "dataset.dropna(inplace=True)\n",
    "dataset.to_csv('Cleaned_Dataset.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning Analysis (Topic Modeling)\n",
    "After preprocessing the text data, we move on to the machine learning analysis part of our NLP project. The goal of this part is to cluster the text responses into different topics based on their content.\n",
    "\n",
    "To accomplish this, we first use a Sentence-BERT model to vectorize the text. The vectorized text is then processed with Principal Component Analysis (PCA) to reduce the dimensionality of the data. Finally, we apply the HDBSCAN clustering algorithm to the reduced-dimensional vectorized text. HDBSCAN is a density-based clustering algorithm that is particularly effective at clustering high-dimensional data. It works by identifying regions of high density in the data and separating them into clusters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Import all the packages needed for topic modeling\n",
    "import pandas as pd\n",
    "import hdbscan\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Read cleaned dataset, we will use the 'Response_Processed' column of the dataset as the input data\n",
    "dataset = pd.read_csv('Cleaned_Dataset.csv')\n",
    "# dataset.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use SentenceBert Encoder to vectorize the response text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Fit the data to the pre-trained model\n",
    "bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "text_responses =  dataset['Response_Processed']\n",
    "sentence_embeddings = bert_model.encode(text_responses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because the dimension of the embedded vector is very large, we perform Principle Component Analysis on the vectors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# Define a variable `dim` to store the desired dimension of the output vector\n",
    "dim = 20\n",
    "# Define the PCA model\n",
    "model = PCA(n_components = dim)\n",
    "reduced_embeddings = model.fit_transform(sentence_embeddings)\n",
    "explained_variance = model.explained_variance_ratio_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "0.717490941286087"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the percentage of the amount of information we retrained from the reduced_embedding vector\n",
    "sum(explained_variance)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we applied HDBSCAN clustering algorithm on the vector space to get a cluster result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose 'l1' norm to calculate the distance, more hyperparameters can be adjusted\n",
    "cluster_model = hdbscan.HDBSCAN(metric='l1')\n",
    "cluster_model.fit(reduced_embeddings)\n",
    "cluster_model.labels_.max()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "dataset['Cluster'] = cluster_model.labels_\n",
    "dataset.to_csv('VS_Result.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We classified the responses into 10 different clusters and store them back into the dataset. We are going to use the cluster result to perform some further analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "This concludes my coding sample. Please note that this coding sample does not show the full research process. However, it does provide a glimpse into the technical aspects of the project and showcases my proficiency in Python and NLP.\n",
    "\n",
    "I hope that this coding sample has been informative and enjoyable for you. If you have any follow-up questions or would like more information about my work, please don't hesitate to contact me. I would be happy to discuss my experience and skills in more detail."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
